{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "clase-2-pre-procesamiento.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/merrecalde/curso_la_plata_2019/blob/master/clase_2_pre_procesamiento.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhxKZmKujgH_",
        "colab_type": "text"
      },
      "source": [
        "# Notebook: clase-2-pre-procesamiento.ipynb\n",
        "#### Se ejemplifican algunas de las tareas de pre-procesamiento descriptas en la clase 2 del curso \"Minería de textos\" "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSCm5V2Ckt0E",
        "colab_type": "text"
      },
      "source": [
        "## 1) Partición de texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VGZcxO3RFq8",
        "colab_type": "code",
        "outputId": "97be0a5f-7328-4c47-d78e-a763dddb1674",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import re\n",
        "\n",
        "raw = \"\"\"'When I'M a Duchess,' she said to herself, (not in a very \n",
        "hopeful tone though), 'I won't have any pepper in my kitchen AT \n",
        "ALL. Soup does very well without--Maybe it's always pepper that \n",
        "makes people hot-tempered,'\"\"\"\n",
        "\n",
        "print(raw.split()) #usando split de strings como herramienta\n",
        "print()\n",
        "print(re.split(r' ', raw)) #con expresiones regulares (ojo) "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in', 'a', 'very', 'hopeful', 'tone', 'though),', \"'I\", \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very', 'well', 'without--Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'\"]\n",
            "\n",
            "[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in', 'a', 'very', '\\nhopeful', 'tone', 'though),', \"'I\", \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', '\\nALL.', 'Soup', 'does', 'very', 'well', 'without--Maybe', \"it's\", 'always', 'pepper', 'that', '\\nmakes', 'people', \"hot-tempered,'\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6_qfMWBRFrB",
        "colab_type": "code",
        "outputId": "5c5f26b4-da8d-4b38-9671-ff9c3791c976",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(re.split(r'[ \\t\\n]+', raw))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in', 'a', 'very', 'hopeful', 'tone', 'though),', \"'I\", \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very', 'well', 'without--Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDy-ZsnLRFrF",
        "colab_type": "code",
        "outputId": "6a5c231f-d3a5-495e-cb3d-6648b70c5676",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(re.split(r'\\s+', raw))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in', 'a', 'very', 'hopeful', 'tone', 'though),', \"'I\", \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very', 'well', 'without--Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SbPjgWMRFrI",
        "colab_type": "code",
        "outputId": "afa975e1-38d5-4395-cef3-af2e4351f061",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(re.split(r'\\W+', raw))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', 'When', 'I', 'M', 'a', 'Duchess', 'she', 'said', 'to', 'herself', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', 'I', 'won', 't', 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', 'Soup', 'does', 'very', 'well', 'without', 'Maybe', 'it', 's', 'always', 'pepper', 'that', 'makes', 'people', 'hot', 'tempered', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9rjn4BZRFrL",
        "colab_type": "code",
        "outputId": "60f92dd0-9fd9-49aa-d297-d0249d000f14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(re.findall(r'\\w+', raw))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['When', 'I', 'M', 'a', 'Duchess', 'she', 'said', 'to', 'herself', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', 'I', 'won', 't', 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', 'Soup', 'does', 'very', 'well', 'without', 'Maybe', 'it', 's', 'always', 'pepper', 'that', 'makes', 'people', 'hot', 'tempered']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn-95lBGRFrP",
        "colab_type": "code",
        "outputId": "a2eb74c8-eb23-4598-c6d5-ae2be373d829",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(re.findall(r'\\w+|\\S\\w*', raw))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"'When\", 'I', \"'M\", 'a', 'Duchess', ',', \"'\", 'she', 'said', 'to', 'herself', ',', '(not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', \"'I\", 'won', \"'t\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup', 'does', 'very', 'well', 'without', '-', '-Maybe', 'it', \"'s\", 'always', 'pepper', 'that', 'makes', 'people', 'hot', '-tempered', ',', \"'\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpvO_LKYRFrS",
        "colab_type": "code",
        "outputId": "142ff488-8ca9-4645-d2e0-e0db82489c06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(re.findall(r\"\\w+(?:[-']\\w+)*|'|[-.(]+|\\S\\w*\", raw))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"'\", 'When', \"I'M\", 'a', 'Duchess', ',', \"'\", 'she', 'said', 'to', 'herself', ',', '(', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', \"'\", 'I', \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup', 'does', 'very', 'well', 'without', '--', 'Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', 'hot-tempered', ',', \"'\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MmbnSxstVv2",
        "colab_type": "text"
      },
      "source": [
        "## 2) Filtrado de palabras\n",
        "Se ejemplifica en otras notebooks junto con la normalización o la vectorización de documentos\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBKWlNbFtyNd",
        "colab_type": "text"
      },
      "source": [
        "## 3) Normalizaciones de palabras\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D6NevHauBSV",
        "colab_type": "text"
      },
      "source": [
        "### 3.1) Lematización y truncado\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnPRf9dqRFrV",
        "colab_type": "code",
        "outputId": "7a364d83-cae0-4849-c2fe-05c160000b0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import spacy\n",
        "import nltk\n",
        "\n",
        "# cargar el modelo del lenguaje inglés de spacy\n",
        "en_nlp = spacy.load('en')\n",
        "doc = u\"I saw there some saws to cut the tree\"\n",
        "# instanciar el \"stemmer\" de Porter de nltk\n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "# tokenizar documento con spacy\n",
        "doc_spacy = en_nlp(doc)\n",
        "# imprimir lemas encontrados por spacy\n",
        "print(\"Lematización:\")\n",
        "print([token.lemma_ for token in doc_spacy])\n",
        "# imprimir tokens obtenidos con el stemmer de Porter\n",
        "print(\"Truncado:\")\n",
        "print([stemmer.stem(token.norm_.lower()) for token in doc_spacy])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lematización:\n",
            "['-PRON-', 'see', 'there', 'some', 'saw', 'to', 'cut', 'the', 'tree']\n",
            "Truncado:\n",
            "['i', 'saw', 'there', 'some', 'saw', 'to', 'cut', 'the', 'tree']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9glIN3YyaRW",
        "colab_type": "text"
      },
      "source": [
        "## 4) Etiquetado\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSqsm4Boyybr",
        "colab_type": "text"
      },
      "source": [
        "### 4.1) Etiquetado de las categorías gramaticales (POS tagging) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_Df8TgtRFrg",
        "colab_type": "code",
        "outputId": "878016b5-f650-4975-f349-63b5978d7267",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import pos_tag, wordpunct_tokenize\n",
        "text = \"The old building was demolished. Tomorrow, they will begin building a new one\"\n",
        "pos_tag(wordpunct_tokenize(text))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'),\n",
              " ('old', 'JJ'),\n",
              " ('building', 'NN'),\n",
              " ('was', 'VBD'),\n",
              " ('demolished', 'VBN'),\n",
              " ('.', '.'),\n",
              " ('Tomorrow', 'NNP'),\n",
              " (',', ','),\n",
              " ('they', 'PRP'),\n",
              " ('will', 'MD'),\n",
              " ('begin', 'VB'),\n",
              " ('building', 'VBG'),\n",
              " ('a', 'DT'),\n",
              " ('new', 'JJ'),\n",
              " ('one', 'CD')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow9d9umkRFrk",
        "colab_type": "code",
        "outputId": "e366d463-74e0-49c9-c17d-b4df92c0a654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "text = \"The old building was demolished. Tomorrow, they will begin building a new one\"\n",
        "pos_tag(word_tokenize(text))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'),\n",
              " ('old', 'JJ'),\n",
              " ('building', 'NN'),\n",
              " ('was', 'VBD'),\n",
              " ('demolished', 'VBN'),\n",
              " ('.', '.'),\n",
              " ('Tomorrow', 'NNP'),\n",
              " (',', ','),\n",
              " ('they', 'PRP'),\n",
              " ('will', 'MD'),\n",
              " ('begin', 'VB'),\n",
              " ('building', 'VBG'),\n",
              " ('a', 'DT'),\n",
              " ('new', 'JJ'),\n",
              " ('one', 'CD')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJJwQg2aRFrn",
        "colab_type": "code",
        "outputId": "9d8a2976-67f6-498e-9278-a0a706fbc5da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "text = \"The grand jury commented on a number of other topics.\"\n",
        "pos_tag(wordpunct_tokenize(text))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'),\n",
              " ('grand', 'JJ'),\n",
              " ('jury', 'NN'),\n",
              " ('commented', 'VBD'),\n",
              " ('on', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('number', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('other', 'JJ'),\n",
              " ('topics', 'NNS'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uyj8BRN8v4N9",
        "colab_type": "text"
      },
      "source": [
        "### 4.2) Desambiguación del sentido de las palabras (WSD)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjWiLOcpRFrq",
        "colab_type": "code",
        "outputId": "727361f7-72f2-480b-8f80-68e6a5a2aebf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "wn.synsets('bass')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('bass.n.01'),\n",
              " Synset('bass.n.02'),\n",
              " Synset('bass.n.03'),\n",
              " Synset('sea_bass.n.01'),\n",
              " Synset('freshwater_bass.n.01'),\n",
              " Synset('bass.n.06'),\n",
              " Synset('bass.n.07'),\n",
              " Synset('bass.n.08'),\n",
              " Synset('bass.s.01')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4SsAljsRFrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Natural Language Toolkit: Word Sense Disambiguation Algorithms\n",
        "#\n",
        "# Authors: Liling Tan <alvations@gmail.com>,\n",
        "#          Dmitrijs Milajevs <dimazest@gmail.com>\n",
        "#\n",
        "# Copyright (C) 2001-2018 NLTK Project\n",
        "# URL: <http://nltk.org/>\n",
        "# For license information, see LICENSE.TXT\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "\n",
        "def lesk(context_sentence, ambiguous_word, pos=None, synsets=None):\n",
        "    \"\"\"Return a synset for an ambiguous word in a context.\n",
        "\n",
        "    :param iter context_sentence: The context sentence where the ambiguous word\n",
        "         occurs, passed as an iterable of words.\n",
        "    :param str ambiguous_word: The ambiguous word that requires WSD.\n",
        "    :param str pos: A specified Part-of-Speech (POS).\n",
        "    :param iter synsets: Possible synsets of the ambiguous word.\n",
        "    :return: ``lesk_sense`` The Synset() object with the highest signature overlaps.\n",
        "\n",
        "    This function is an implementation of the original Lesk algorithm (1986) [1].\n",
        "\n",
        "    Usage example::\n",
        "\n",
        "        >>> lesk(['I', 'went', 'to', 'the', 'bank', 'to', 'deposit', 'money', '.'], 'bank', 'n')\n",
        "        Synset('savings_bank.n.02')\n",
        "\n",
        "    [1] Lesk, Michael. \"Automatic sense disambiguation using machine\n",
        "    readable dictionaries: how to tell a pine cone from an ice cream\n",
        "    cone.\" Proceedings of the 5th Annual International Conference on\n",
        "    Systems Documentation. ACM, 1986.\n",
        "    http://dl.acm.org/citation.cfm?id=318728\n",
        "    \"\"\"\n",
        "\n",
        "    context = set(context_sentence)\n",
        "    if synsets is None:\n",
        "        synsets = wordnet.synsets(ambiguous_word)\n",
        "\n",
        "    if pos:\n",
        "        synsets = [ss for ss in synsets if str(ss.pos()) == pos]\n",
        "\n",
        "    if not synsets:\n",
        "        return None\n",
        "\n",
        "    _, sense = max(\n",
        "        (len(context.intersection(ss.definition().split())), ss) for ss in synsets\n",
        "    )\n",
        "\n",
        "    return sense\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiiH-Ix2RFrv",
        "colab_type": "code",
        "outputId": "32386a36-0ab3-4aa9-a8d9-73b2e3122c70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lesk(['I', 'went', 'to', 'the', 'bank', 'to', 'deposit', 'money', '.'], 'bank', 'n')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Synset('savings_bank.n.02')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEHuGT8cyTEh",
        "colab_type": "text"
      },
      "source": [
        "### 4.3) Reconocimiento de entidades nombradas (NER)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zONJRU4xRFry",
        "colab_type": "code",
        "outputId": "95ddaeea-0b32-4248-d391-b379a365182c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        }
      },
      "source": [
        "nltk.download('treebank')\n",
        "sent = nltk.corpus.treebank.tagged_sents()[22]\n",
        "sent"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'),\n",
              " ('U.S.', 'NNP'),\n",
              " ('is', 'VBZ'),\n",
              " ('one', 'CD'),\n",
              " ('of', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('few', 'JJ'),\n",
              " ('industrialized', 'VBN'),\n",
              " ('nations', 'NNS'),\n",
              " ('that', 'WDT'),\n",
              " ('*T*-7', '-NONE-'),\n",
              " ('does', 'VBZ'),\n",
              " (\"n't\", 'RB'),\n",
              " ('have', 'VB'),\n",
              " ('a', 'DT'),\n",
              " ('higher', 'JJR'),\n",
              " ('standard', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('regulation', 'NN'),\n",
              " ('for', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('smooth', 'JJ'),\n",
              " (',', ','),\n",
              " ('needle-like', 'JJ'),\n",
              " ('fibers', 'NNS'),\n",
              " ('such', 'JJ'),\n",
              " ('as', 'IN'),\n",
              " ('crocidolite', 'NN'),\n",
              " ('that', 'WDT'),\n",
              " ('*T*-1', '-NONE-'),\n",
              " ('are', 'VBP'),\n",
              " ('classified', 'VBN'),\n",
              " ('*-5', '-NONE-'),\n",
              " ('as', 'IN'),\n",
              " ('amphobiles', 'NNS'),\n",
              " (',', ','),\n",
              " ('according', 'VBG'),\n",
              " ('to', 'TO'),\n",
              " ('Brooke', 'NNP'),\n",
              " ('T.', 'NNP'),\n",
              " ('Mossman', 'NNP'),\n",
              " (',', ','),\n",
              " ('a', 'DT'),\n",
              " ('professor', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('pathlogy', 'NN'),\n",
              " ('at', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('University', 'NNP'),\n",
              " ('of', 'IN'),\n",
              " ('Vermont', 'NNP'),\n",
              " ('College', 'NNP'),\n",
              " ('of', 'IN'),\n",
              " ('Medicine', 'NNP'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nP5VFGJvRFr1",
        "colab_type": "code",
        "outputId": "5d8870d6-d6fe-4b57-ced2-88fe00d1f8f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1003
        }
      },
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "print(nltk.ne_chunk(sent))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "(S\n",
            "  The/DT\n",
            "  (GPE U.S./NNP)\n",
            "  is/VBZ\n",
            "  one/CD\n",
            "  of/IN\n",
            "  the/DT\n",
            "  few/JJ\n",
            "  industrialized/VBN\n",
            "  nations/NNS\n",
            "  that/WDT\n",
            "  *T*-7/-NONE-\n",
            "  does/VBZ\n",
            "  n't/RB\n",
            "  have/VB\n",
            "  a/DT\n",
            "  higher/JJR\n",
            "  standard/NN\n",
            "  of/IN\n",
            "  regulation/NN\n",
            "  for/IN\n",
            "  the/DT\n",
            "  smooth/JJ\n",
            "  ,/,\n",
            "  needle-like/JJ\n",
            "  fibers/NNS\n",
            "  such/JJ\n",
            "  as/IN\n",
            "  crocidolite/NN\n",
            "  that/WDT\n",
            "  *T*-1/-NONE-\n",
            "  are/VBP\n",
            "  classified/VBN\n",
            "  *-5/-NONE-\n",
            "  as/IN\n",
            "  amphobiles/NNS\n",
            "  ,/,\n",
            "  according/VBG\n",
            "  to/TO\n",
            "  (PERSON Brooke/NNP T./NNP Mossman/NNP)\n",
            "  ,/,\n",
            "  a/DT\n",
            "  professor/NN\n",
            "  of/IN\n",
            "  pathlogy/NN\n",
            "  at/IN\n",
            "  the/DT\n",
            "  (ORGANIZATION University/NNP)\n",
            "  of/IN\n",
            "  (PERSON Vermont/NNP College/NNP)\n",
            "  of/IN\n",
            "  (GPE Medicine/NNP)\n",
            "  ./.)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}